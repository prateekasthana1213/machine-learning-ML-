{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoW9YNEu3aay"
      },
      "outputs": [],
      "source": [
        "'''Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer: Information Gain (IG) is a metric used in Decision Trees to decide which feature should be used to split the data at each node.\n",
        "        Information Gain measures the reduction in uncertainty (entropy) about the target variable after splitting the dataset based on a particular feature.\n",
        "\n",
        "        In simple words, it tells us how much information a feature provides about the class label.\n",
        "\n",
        " Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        " Answer: Gini Impurity measures the probability of misclassifying a randomly chosen data point if it were labeled according to the class distribution of the node.\n",
        "\n",
        "          Entropy measures the uncertainty or randomness in the data using information theory.\n",
        "          Gini Impurity\t0 to 0.5 (binary classification)\n",
        "          Entropy\t0 to 1 (binary classification)\n",
        "\n",
        "           Gini Impurity\n",
        "\n",
        "      Faster to compute\n",
        "      Focuses on misclassification probability\n",
        "      Slightly biased toward larger classes\n",
        "\n",
        "      Entropy\n",
        "\n",
        "     Based on information theory\n",
        "     Measures disorder/uncertainty\n",
        "     More sensitive to changes in probability\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:Pre-Pruning (also called early stopping) is a technique used in decision trees to stop the tree from growing further before it perfectly fits the training data.\n",
        "       The goal is to prevent overfitting and improve the model’s ability to generalize to unseen data.'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 4:Write a Python program to train a Decision Tree Classifier using Gini'''\n",
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, dt_model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB9EniSKY3_-",
        "outputId": "654533c4-0135-44d2-bff3-4cdf813f18a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 5: What is a Support Vector Machine (SVM)?\n",
        "Answer:A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        " Its main objective is to find an optimal decision boundary (hyperplane) that maximizes the margin between different classes in the feature space.\n",
        "\n",
        " Question 6: What is the Kernel Trick in SVM?\n",
        "Answer:The Kernel Trick is a technique used in Support Vector Machines (SVMs) that allows the algorithm to solve non-linearly separable problems by\n",
        "      implicitly mapping data into a higher-dimensional feature space, where a linear separation becomes possible, without explicitly computing the transformation.'''"
      ],
      "metadata": {
        "id": "FW3pz0zmY4r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.'\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Linear SVM\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAFU29kvaN53",
        "outputId": "3cae7097-057d-405a-8474-e1f72614ff32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9722222222222222\n",
            "RBF SVM Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "Answer:The Naïve Bayes classifier is a supervised probabilistic machine learning algorithm based on Bayes’ Theorem.\n",
        "It is mainly used for classification tasks, especially in text classification, spam detection, and sentiment analysis.\n",
        "It is called “Naïve” because it makes a strong assumption that:\n",
        "\n",
        "All features are conditionally independent given the class label\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "Answer:Gaussian Naïve Bayes (GNB)\n",
        "\n",
        "Type of data: Continuous numerical data\n",
        "\n",
        "Probability model: Gaussian (mean & variance per feature per class)\n",
        "\n",
        "Examples: Height, weight, salary, temperature\n",
        "\n",
        "Use cases: Medical diagnosis, sensor data, real-valued datasets\n",
        ". Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "Type of data: Discrete count data\n",
        "\n",
        "Probability model: Word or event frequencies\n",
        "\n",
        "Examples: Word counts in documents, number of occurrences\n",
        "\n",
        "Use cases: Text classification, spam detection, document categorization\n",
        "\n",
        "Bernoulli Naïve Bayes (BNB)\n",
        "\n",
        "Type of data: Binary (0 or 1)\n",
        "\n",
        "Probability model: Presence or absence of features\n",
        "\n",
        "Examples: Word appears or not in a document\n",
        "\n",
        "Use cases: Text classification with binary features"
      ],
      "metadata": {
        "id": "4fl_v0Iqam7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM83SYYHw_hX",
        "outputId": "b1c10a61-6522-423a-f064-3b65e55b2600"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n",
            "\n",
            "Confusion Matrix:\n",
            " [[40  3]\n",
            " [ 0 71]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    }
  ]
}