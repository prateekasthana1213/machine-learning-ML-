{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3895MYDRB6G"
      },
      "outputs": [],
      "source": [
        "'''Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "Answer: Ensemble Learning is a machine learning technique in which multiple models (called base learners or weak learners) are trained and combined to make a single,\n",
        "stronger predictive model.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "\n",
        "The core idea is that a group of models can perform better than any single model alone. Different models may make different errors; by combining them,\n",
        "these errors can cancel out, leading to better accuracy, robustness, and generalization.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "Answer: Bagging\n",
        "- models are trained independantly\n",
        "-uses bootstrap sampling (sampling with replacement)\n",
        "-no dependency among models\n",
        "-Reduce variance by averaging\n",
        "-all data points treat equally\n",
        "\n",
        "       Boosting\n",
        "-models are trained sequentially\n",
        "-uses weighted sampling\n",
        "-each models are depends on the previous one\n",
        "-reduce bias and improve weak learner\n",
        "-misclasified data points are given higher weight\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "Answer:Bootstrap sampling is a resampling technique in which multiple new training datasets are created by randomly sampling from the original dataset with replacement.\n",
        "Each bootstrap sample has the same size as the original dataset, but some data points may appear multiple times while others may be left out.\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "In Bagging (Bootstrap Aggregating) methods such as Random Forest, bootstrap sampling plays a crucial role:\n",
        "\n",
        "Creates diverse training sets\n",
        "\n",
        "Each model (e.g., each decision tree in a Random Forest) is trained on a different bootstrap sample.\n",
        "\n",
        "This introduces variation among models.\n",
        "\n",
        "Reduces variance\n",
        "\n",
        "Individual models may overfit, but averaging their predictions reduces variance and improves generalization.\n",
        "\n",
        "Improves model robustness\n",
        "\n",
        "Since models see slightly different data, the ensemble becomes less sensitive to noise.\n",
        "\n",
        "Out-of-Bag (OOB) estimation\n",
        "\n",
        "About 63% of unique data points appear in each bootstrap sample.\n",
        "\n",
        "The remaining 37% (out-of-bag samples) are used to estimate model performance without a separate validation set.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "Answer:Out-of-Bag (OOB) samples are the data points from the original dataset that are not selected in a particular bootstrap sample during training.\n",
        "\n",
        "When bootstrap sampling is used (as in Bagging and Random Forest):\n",
        "\n",
        "Each model is trained on a bootstrap sample of the dataset.\n",
        "\n",
        "On average, about 63% of the data points are included in a bootstrap sample.\n",
        "\n",
        "The remaining 37% are called Out-of-Bag (OOB) samples for that model.\n",
        "\n",
        "teps to compute OOB score:\n",
        "\n",
        "For each data point, collect predictions only from the models where this point was an OOB sample.\n",
        "\n",
        "Aggregate these predictions:\n",
        "\n",
        "Majority voting for classification\n",
        "\n",
        "Averaging for regression\n",
        "\n",
        "Compare the aggregated prediction with the true label.\n",
        "\n",
        "Compute an evaluation metric:\n",
        "\n",
        "Accuracy for classification\n",
        "\n",
        "Mean Squared Error (MSE) or R² for regression\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "Answer:          Single Decision Tree\n",
        "\n",
        "-Computed from impurity reduction (Gini or Entropy) at splits in one tree\n",
        "-Unstable – small data changes can greatly alter importance\n",
        "-Can be biased toward dominant features\n",
        "-High risk of overfitting affects importance values\n",
        "\n",
        "               Random Forest\n",
        "\n",
        "-Averaged impurity reduction across many trees\n",
        "-More stable and reliable due to averaging\n",
        "-Reduces bias by combining multiple trees\n",
        "-Lower overfitting → more trustworthy importance'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance (descending)\n",
        "feature_importance_df = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ")\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRtsbp5YX2LJ",
        "outputId": "8f7a0711-1dc5-4b91-f2d5-62f256de21c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_predictions = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_predictions = bagging.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_predictions)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3S7q3NYNIX",
        "outputId": "67b84dc8-e8bc-4b92-dc1e-c970aa50b68c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate final accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Final Test Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqN4NzJIaTBv",
        "outputId": "49e048a8-ec50-4638-fc1d-988726e18c00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Test Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)'''\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE for Bagging Regressor\n",
        "bagging_predictions = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE for Random Forest Regressor\n",
        "rf_predictions = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4IXB2zHbCey",
        "outputId": "05bed984-213b-414c-f3b2-1cfad9a0ef98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2568358813508342\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Create a synthetic loan default dataset\n",
        "# -------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    n_classes=2,\n",
        "    weights=[0.7, 0.3],   # class imbalance (non-default vs default)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Define base and ensemble models\n",
        "# -------------------------------------------------\n",
        "# Base model\n",
        "dt = DecisionTreeClassifier(\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Bagging-based ensemble (Random Forest)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Boosting-based ensemble\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Cross-validation setup\n",
        "# -------------------------------------------------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Evaluate models using ROC-AUC\n",
        "# -------------------------------------------------\n",
        "dt_scores = cross_val_score(dt, X, y, cv=cv, scoring='roc_auc')\n",
        "rf_scores = cross_val_score(rf, X, y, cv=cv, scoring='roc_auc')\n",
        "gb_scores = cross_val_score(gb, X, y, cv=cv, scoring='roc_auc')\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Print results\n",
        "# -------------------------------------------------\n",
        "print(\"Decision Tree ROC-AUC:\", np.mean(dt_scores))\n",
        "print(\"Random Forest (Bagging) ROC-AUC:\", np.mean(rf_scores))\n",
        "print(\"Gradient Boosting ROC-AUC:\", np.mean(gb_scores))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnmf55hKbxds",
        "outputId": "bb772483-b9ce-4174-b3b7-a886ae23481c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree ROC-AUC: 0.8824812505013269\n",
            "Random Forest (Bagging) ROC-AUC: 0.9730804289868275\n",
            "Gradient Boosting ROC-AUC: 0.96505493330973\n"
          ]
        }
      ]
    }
  ]
}